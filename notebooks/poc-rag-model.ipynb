{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r session/install-deps/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from huggingface_hub import hf_hub_download\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# Define the model \n",
    "GEN_AI_MODEL_REPO = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
    "GEN_AI_MODEL_FILENAME = \"llama-2-13b-chat.Q5_0.gguf\"\n",
    "\n",
    "print(\"Initiate Llama model...\")\n",
    "def load_llama_model():\n",
    "    gen_ai_model_path = hf_hub_download(repo_id=GEN_AI_MODEL_REPO, filename=GEN_AI_MODEL_FILENAME)\n",
    "    print(\"path is:\")\n",
    "    print(gen_ai_model_path)\n",
    "    llama2_model = Llama(\n",
    "        model_path=gen_ai_model_path,\n",
    "        n_gpu_layers=20,\n",
    "        n_ctx=20\n",
    "    )\n",
    "    return llama2_model\n",
    "\n",
    "\n",
    "llama2_model = load_llama_model()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Pass through user input to LLM model with enhanced prompt and stop tokens\n",
    "def generate_response(json_input):\n",
    "\n",
    "    try:\n",
    "        print(\"Input from user: \")\n",
    "        print(json_input)\n",
    "        # Assuming json_input is your dictionary\n",
    "        json_input_str = json.dumps(json_input)\n",
    "        data = json.loads(json_input_str)\n",
    "        print(\"json.loads:\")\n",
    "        print(data)\n",
    "        question = \"Answer this question based on given context: \" + data['prompt'] + \" \"\n",
    "        context = \" Here is the context: \" + str(data['context'])\n",
    "        question_and_context = question + context\n",
    "\n",
    "        params = {\n",
    "            \"temperature\": float(data['temperature']),\n",
    "            \"max_tokens\": int(data['max_tokens'])\n",
    "        }\n",
    "        response = llama2_model(prompt=question_and_context, **params)\n",
    "\n",
    "        model_out = response['choices'][0]['text']\n",
    "\n",
    "        print(\"Response from Llama model: \")\n",
    "        print(response)\n",
    "        print(\"--------------------------- \")\n",
    "        print(model_out)\n",
    "        print(\"--------------------------- \")\n",
    "\n",
    "        # answer = response['choices'][0]['text']\n",
    "        return {\"response\": \"Success\"}\n",
    "    \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input data\n",
    "input_data = {\n",
    "    \"prompt\": \"What is Cloudera Machine Learning?\",\n",
    "    \"temperature\": 1,\n",
    "    \"max_tokens\": 50,\n",
    "    \"context\": \"Cloudera Machine Learning is a platform for machine learning and analytics that runs in the public cloud or on-premises.\",\n",
    "    \"user\": \"genius\"\n",
    "}\n",
    "\n",
    "# Call the function with the input data\n",
    "result = generate_response(input_data)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
